# Instrucciones Backend: Manejo de Archivos Grandes y Error 502

## üî¥ Problema Actual

El backend est√° devolviendo **502 Bad Gateway** al intentar subir archivos Excel grandes (~19MB):

```
POST /listas-comparativas/excel
Status Code: 502 Bad Gateway
Content-Length: 19496090 (19MB)
```

## ‚úÖ Soluciones

### 1. Aumentar L√≠mites de Tama√±o en FastAPI

En el archivo principal (`main.py` o `app.py`):

```python
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI()

# Configurar l√≠mites de tama√±o de archivo
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "https://rapifarma-administrativo.vercel.app",
        "http://localhost:5173",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Aumentar l√≠mite de tama√±o de request
@app.middleware("http")
async def increase_request_size(request: Request, call_next):
    # Permitir archivos hasta 50MB
    if request.headers.get("content-type", "").startswith("multipart/form-data"):
        # FastAPI maneja multipart autom√°ticamente, pero necesitamos configurar uvicorn
        pass
    response = await call_next(request)
    return response
```

### 2. Configurar Uvicorn con L√≠mites Mayores

En el archivo de configuraci√≥n de uvicorn o en el comando de inicio:

```python
# Si usas uvicorn.run() en el c√≥digo
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=8000,
        limit_concurrency=1000,
        limit_max_requests=1000,
        timeout_keep_alive=75,
        # No hay l√≠mite directo en uvicorn para tama√±o de body
        # Se maneja en FastAPI
    )
```

O en el comando de Render.com, aseg√∫rate de que no haya l√≠mites:

```bash
uvicorn app.main:app --host 0.0.0.0 --port $PORT --timeout-keep-alive 75
```

### 3. Configurar el Endpoint para Archivos Grandes

En el router de `listas-comparativas`:

```python
from fastapi import APIRouter, UploadFile, File, Form, Depends
from fastapi.responses import JSONResponse

router = APIRouter()

@router.post("/excel")
async def subir_lista_excel(
    archivo: UploadFile = File(..., max_length=52428800),  # 50MB m√°ximo
    proveedorId: str = Form(...),
    usuarioCorreo: str = Form(...),
    # ... otros par√°metros
):
    try:
        # Leer el archivo en chunks para no cargar todo en memoria
        contents = await archivo.read()
        
        # Procesar el archivo
        # ... tu l√≥gica aqu√≠
        
        return {"message": "Lista subida exitosamente", "items_procesados": len(items)}
    except Exception as e:
        # Log del error
        print(f"Error al procesar archivo: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"detail": f"Error al procesar archivo: {str(e)}"}
        )
```

### 4. Procesar Archivos en Chunks (Recomendado para Archivos Muy Grandes)

```python
import io
import openpyxl
from openpyxl import load_workbook

@router.post("/excel")
async def subir_lista_excel(
    archivo: UploadFile = File(...),
    proveedorId: str = Form(...),
    usuarioCorreo: str = Form(...),
):
    try:
        # Leer el archivo
        contents = await archivo.read()
        
        # Usar BytesIO para no escribir en disco
        excel_file = io.BytesIO(contents)
        
        # Cargar workbook con data_only=True para leer valores, no f√≥rmulas
        workbook = load_workbook(excel_file, data_only=True, read_only=True)
        sheet = workbook.active
        
        items = []
        batch_size = 1000  # Procesar en lotes de 1000
        
        # Leer filas en chunks
        for row_idx, row in enumerate(sheet.iter_rows(min_row=2, values_only=True), start=2):
            if row_idx % batch_size == 0:
                # Insertar lote en la base de datos
                if items:
                    db.listas_precios_proveedores.insert_many(items)
                    items = []
            
            # Procesar fila
            # ... tu l√≥gica aqu√≠
            items.append(item_data)
        
        # Insertar √∫ltimo lote
        if items:
            db.listas_precios_proveedores.insert_many(items)
        
        workbook.close()
        
        return {"message": "Lista subida exitosamente"}
    except Exception as e:
        print(f"Error: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"detail": str(e)}
        )
```

### 5. Configurar Timeout en Render.com

En Render.com, verifica:
1. **Service Settings** ‚Üí **Health Check Path**: Debe estar configurado
2. **Environment Variables**: No debe haber l√≠mites de tama√±o
3. **Auto-Deploy**: Verifica que el servicio no se est√© reiniciando durante la subida

### 6. Solucionar Error de bcrypt

El error en los logs:
```
AttributeError: module 'bcrypt' has no attribute '__about__'
```

**Soluci√≥n:**

```bash
# Actualizar bcrypt y passlib
pip install --upgrade bcrypt passlib
```

O en `requirements.txt`:
```
bcrypt>=4.0.0
passlib[bcrypt]>=1.7.4
```

### 7. Optimizar el Procesamiento

Para archivos grandes, considera:

1. **Procesar as√≠ncronamente:**
```python
from fastapi import BackgroundTasks

@router.post("/excel")
async def subir_lista_excel(
    archivo: UploadFile = File(...),
    proveedorId: str = Form(...),
    background_tasks: BackgroundTasks = BackgroundTasks(),
):
    # Guardar archivo temporalmente
    file_path = f"/tmp/{archivo.filename}"
    with open(file_path, "wb") as f:
        contents = await archivo.read()
        f.write(contents)
    
    # Procesar en background
    background_tasks.add_task(procesar_archivo_async, file_path, proveedorId)
    
    return {"message": "Archivo recibido, procesando en background..."}
```

2. **Usar bulk operations de MongoDB:**
```python
from pymongo import InsertOne, UpdateOne

operations = []
for item in items:
    operations.append(InsertOne(item))

if operations:
    db.listas_precios_proveedores.bulk_write(operations, ordered=False)
```

## üìã Checklist

- [ ] Aumentar l√≠mite de tama√±o de archivo en FastAPI
- [ ] Configurar uvicorn sin l√≠mites de timeout
- [ ] Procesar archivos en chunks/lotes
- [ ] Actualizar bcrypt y passlib
- [ ] Verificar configuraci√≥n de Render.com
- [ ] Agregar logging para debug
- [ ] Manejar errores con try/catch apropiados
- [ ] Usar bulk operations de MongoDB

## üß™ Pruebas

Despu√©s de implementar:

1. Probar con archivo peque√±o (< 1MB)
2. Probar con archivo mediano (5-10MB)
3. Probar con archivo grande (15-20MB)
4. Verificar logs del servidor
5. Verificar que los datos se guarden correctamente

## ‚ö†Ô∏è Notas Importantes

- **Render.com** puede tener l√≠mites de memoria. Si el archivo es muy grande, considera aumentar el plan.
- **MongoDB** puede tardar en insertar muchos documentos. Usa `bulk_write` con `ordered=False` para mejor rendimiento.
- **Timeout**: Aseg√∫rate de que el timeout del servidor sea suficiente (al menos 5 minutos para archivos grandes).

